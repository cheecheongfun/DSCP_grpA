{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2db7b5a1-fa8d-4671-b9e0-df0f9b3c8ebc",
   "metadata": {},
   "source": [
    "<b>This Jupyter Notebook is ran on local environment<b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12154818-1966-40ea-9b67-ec67f44d9d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e30f9a04-d0ad-48ef-9a1c-10f1c2fc8c62",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2eb0873eced04919ba606465a35e56c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langchain.llms import HuggingFacePipeline\n",
    "import torch\n",
    "from  transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, AutoModelForSeq2SeqLM, BitsAndBytesConfig\n",
    "from langchain import PromptTemplate, HuggingFaceHub, LLMChain\n",
    "\n",
    "model_id = 'ccfai/squirmy-the-chatbot-v4'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, load_in_8bit=True)\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=1024,\n",
    "    torch_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "local_llm = HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965518d7-badc-4c46-a57c-9b68f4c686fa",
   "metadata": {},
   "source": [
    "<b>Running chatbot without Gradio Interface<b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbba53a9-e54a-4740-b60b-4fb248d45fc8",
   "metadata": {},
   "source": [
    "Different prompting approach used here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7ec5ce-6703-40fc-8baa-dc572e4eaab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"<s>[INST] <<SYS>>\n",
    "You are a friendly expert in the field of vermicomposting. Please ensure that your responses are clear and concise.\n",
    "Your task is to assist vermicompost enthusiast on answering their questions regarding vermicompost.\n",
    "Note that your target audience are based in Singapore so do consider the geometrical factors of the location when answering.\n",
    "If possible, make sure your answers are summarized, preferably in bullet-points.\n",
    "<</SYS>>\n",
    "{question} [/INST]\n",
    "Bot:\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a56eb9-ca81-4a6a-9f33-e8ccefd3a559",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "\n",
    "llm_chain = LLMChain(prompt=prompt, llm=local_llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e0bed1-8e25-4310-a1fe-400a6025f688",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_chatbot():\n",
    "    while True:\n",
    "        u_input = input(\"You: \").strip()\n",
    "\n",
    "        if u_input.lower()==\"bye\":\n",
    "            print(\"Squirmy: Happy composting! Goodbye!\")\n",
    "            break\n",
    "        response = llm_chain.run(u_input)\n",
    "        model_output=response.split(\"Bot:\")[1].strip()\n",
    "        print(\"Squirmy:\"+\"\\n\"+model_output)\n",
    "\n",
    "print(\"Squirmy: Hello! My name is Squirmy! Ask me anything about composting, or type 'bye' to end the chat.\")\n",
    "run_chatbot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143efbcf-0d89-4c8e-adc0-a17a53cc3e26",
   "metadata": {},
   "source": [
    "<b>With Gradio Interface<b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7fd3e203-8035-4e83-a45b-0b5ee470a603",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "{question}\n",
    "Bot:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12707e52-7569-4334-9f6b-47179edf04d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\envs\\torch\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:119: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 0.3.0. Use RunnableSequence, e.g., `prompt | llm` instead.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "\n",
    "llm_chain = LLMChain(prompt=prompt, llm=local_llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d246eb-fac3-4fd2-8499-f6eba3ad6130",
   "metadata": {},
   "source": [
    "different approach to system prompting + make use of chat history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b727d66c-7e5f-405b-b6be-4a8147cfc2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"<s>[INST] <<SYS>>\n",
    "You are a friendly chatbot here to help with all things vermicomposting for your audience in Singapore.\n",
    "Let's keep the answers clear and concise, and you'll make sure to consider factors relevant to Singapore's conditions in your responses. \n",
    "Always consider keeping the worm in a healthy state at all times.\n",
    "<</SYS>>\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Formatting function for message and history\n",
    "def format_message(message: str, history: list, memory_limit: int = 3) -> str:\n",
    "    \"\"\"\n",
    "    Formats the message and history for the Llama model.\n",
    "\n",
    "    Parameters:\n",
    "        message (str): Current message to send.\n",
    "        history (list): Past conversation history.\n",
    "        memory_limit (int): Limit on how many past interactions to consider.\n",
    "\n",
    "    Returns:\n",
    "        str: Formatted message string\n",
    "    \"\"\"\n",
    "    # always keep len(history) <= memory_limit\n",
    "    if len(history) > memory_limit:\n",
    "        history = history[-memory_limit:]\n",
    "\n",
    "    if len(history) == 0:\n",
    "        return SYSTEM_PROMPT + f\"{message} [/INST]\"\n",
    "\n",
    "    formatted_message = SYSTEM_PROMPT + f\"{history[0][0]} [/INST] {history[0][1]} </s>\"\n",
    "\n",
    "    # Handle conversation history\n",
    "    for user_msg, model_answer in history[1:]:\n",
    "        formatted_message += f\"<s>[INST] {user_msg} [/INST] {model_answer} </s>\"\n",
    "\n",
    "    # Handle the current message\n",
    "    formatted_message += f\"<s>[INST] {message} [/INST]\"\n",
    "\n",
    "    return formatted_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "816464b2-6ecf-4f37-94f9-c291cb8a0d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a response from the Llama model\n",
    "def get_llama_response(message: str, history: list) -> str:\n",
    "\n",
    "    query = format_message(message, history)\n",
    "    \n",
    "    response = llm_chain.run(query)\n",
    "\n",
    "    response = response.split(\"Bot:\")[1].strip()\n",
    "\n",
    "    print(\"Squirmy:\", response.strip())\n",
    "    return response.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4b919f4-e807-4319-adad-b75250ae1db7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "IMPORTANT: You are using gradio version 4.28.3, however version 4.29.0 is available, please upgrade.\n",
      "--------\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\envs\\torch\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:119: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n",
      "C:\\Users\\User\\anaconda3\\envs\\torch\\lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:671: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:263.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Squirmy: Hey there, no worries! 😊 A bad smell in your worm bin is quite common, especially in the early stages of vermicomposting. There are a few reasons why your tank might be smelling bad:\n",
      "\n",
      "1. Over-moisture: If your bin is too wet, anaerobic bacteria can thrive, causing a strong, unpleasant odor. Check your bin's moisture level by feeling the top inch of soil. It should be damp but not waterlogged. Adjust your watering schedule if necessary.\n",
      "2. Food quality: Worms can't digest certain foods, like onions, garlic, and meat, which can cause a strong smell. Make sure to only add foods that are safe for worms, like vegetable scraps, fruit peels, and tea bags.\n",
      "3. Lack of aeration: Worms need oxygen to survive, so if your bin doesn't have enough airflow, anaerobic bacteria can thrive, leading to bad odors. Make sure your bin has adequate airflow by adding some holes or drilling some air vents.\n",
      "4. Overcrowding: If your bin is too full, the worms might not be able to move around and digest food properly, leading to a buildup of food and moisture, which can cause bad odors. Consider adding more bedding or reducing the amount of food you add.\n",
      "\n",
      "To address the bad smell, try these tips:\n",
      "\n",
      "1. Add more bedding: Increasing the amount of bedding material (e.g., shredded newspaper, coconut coir, or straw) can help absorb excess moisture and reduce odors.\n",
      "2. Mix the bin: Give your bin a good mix to ensure the worms are distributed evenly and can access all the food.\n",
      "3. Add a small amount of baking soda: Baking soda can help neutralize odors. Add a teaspoon or two to the bin and mix well.\n",
      "4. Check for pests: If your bin is infested with pests like fruit flies or ants, they could be the source of the bad smell. Check for any signs of pests and take action to eliminate them.\n",
      "\n",
      "Remember, a small amount of odor is normal, but if the smell persists or becomes overpowering, it's time to take action. 😊\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "interface = gr.ChatInterface(fn=get_llama_response, title=\"Squirmy the Composting Chatbot🐛\", description=\"Ask Squirmy anything about composting!\")\n",
    "interface.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.12 (torch)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
