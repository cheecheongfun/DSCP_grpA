{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e4fae06",
   "metadata": {},
   "source": [
    "<table class=\"table table-bordered\">\n",
    "    <tr>\n",
    "        <th style=\"width:250px\"><img src='https://www.np.edu.sg/PublishingImages/Pages/default/odp/ICT.jpg' style=\"width: 100%; height: 125px; \"></th>\n",
    "        <th style=\"text-align:center;\"><h1>Data Science Capstone</h1><h2>Assignment 1 </h2><h3>Diploma in Data Science</h3></th>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061bd7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the required packages\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visual\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from datetime import datetime\n",
    "import nltk\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures, MinMaxScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.svm import SVR\n",
    "sns.set(style=\"white\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d451405",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LAST UPDATED 6/5/24\n",
    "mp1_231223 = pd.read_csv('data/MacPherson_Rack_01 -  23 December 2023.csv')\n",
    "mp1_050424 = pd.read_csv('data/MacPherson_Rack_01 - 5 April 2024.csv')\n",
    "mp1_060424 = pd.read_csv('data/MacPherson_Rack_01 - 6 April 2024.csv')\n",
    "mp1_100424 = pd.read_csv('data/MacPherson_Rack_01 - 10 April 2024.csv')\n",
    "mp1_140424 = pd.read_csv('data/MacPherson_Rack_01 - 14 April 2024.csv')\n",
    "mp1_150424 = pd.read_csv('data/MacPherson_Rack_01 - 15 April 2024.csv')\n",
    "mp1_160424 = pd.read_csv('data/MacPherson_Rack_01 - 16 April 2024.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0dacb77",
   "metadata": {},
   "source": [
    "### 1. Import files "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "093f3729",
   "metadata": {},
   "source": [
    "### Macpherson - 23 Dec 2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d596f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp1_231223.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c17fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp1_231223.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3731b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting column name for appending files together later \n",
    "mp1_231223 = mp1_231223.rename(columns={'Buzzer (NIL)': 'Buzzer'})\n",
    "mp1_231223"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806085c4",
   "metadata": {},
   "source": [
    "### Macpherson - 05 April 2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dda6243",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp1_050424.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683d5189",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp1_050424.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d6fcc4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "mp1_050424"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "748443f5",
   "metadata": {},
   "source": [
    "### Macpherson - 06 April 2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff97ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp1_060424.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677f5e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp1_060424.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42264ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#first 7 rows have '?' for NPK values\n",
    "mp1_060424"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298ee72e",
   "metadata": {},
   "source": [
    "### Macpherson - 10 April 2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d0f596",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp1_100424.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb828a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp1_100424.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b1b48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp1_100424"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f660e4",
   "metadata": {},
   "source": [
    "### Macpherson - 14 April 2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161b5190",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp1_140424.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdaa3578",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp1_140424.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20acf91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp1_140424"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc164758",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping the first column \n",
    "mp1_140424.drop(0, inplace=True) # Row of units and measurements\n",
    "mp1_140424.reset_index(drop=True, inplace=True)\n",
    "mp1_140424"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c955ca80",
   "metadata": {},
   "source": [
    "### Macpherson - 15 April 2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c2c8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp1_150424.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e6913a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp1_150424.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc396c24",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "mp1_150424.drop(0, inplace=True) # Row of units and measurements\n",
    "mp1_150424.reset_index(drop=True, inplace=True)\n",
    "mp1_150424"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a928d1f6",
   "metadata": {},
   "source": [
    "### Macpherson - 16 April 2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81bd103c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp1_160424.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb405c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "mp1_160424.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eca1646",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "mp1_160424.drop(0, inplace=True) # Row of units and measurements\n",
    "mp1_160424.reset_index(drop=True, inplace=True)\n",
    "mp1_160424"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83fad50",
   "metadata": {},
   "source": [
    "### 2. Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5fafcfe",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#appending files together\n",
    "df = pd.concat([mp1_231223, mp1_050424, mp1_060424, mp1_100424, mp1_140424, mp1_150424, mp1_160424], axis=0)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279c215e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fbe017b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106a4d3a",
   "metadata": {},
   "source": [
    "### Exploring objects "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48bee2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exploring grams of compost harvest but its not very useful to analyse  as there are too many columns with null values\n",
    "#Since compost notes only has 1 value, we will not explore it too\n",
    "\n",
    "label_freq = df['Grams of Compost Harvested'].value_counts() / len(df)\n",
    "fig = label_freq.sort_values(ascending=False).plot.bar()\n",
    "\n",
    "fig.axhline(y=0.05, color='red')\n",
    "\n",
    "fig.set_ylabel('Percentage of items in each category')\n",
    "fig.set_xlabel('Variable: Grams of Compost Harvested')\n",
    "fig.set_title('Rare Categories')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1b8db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exporing number of worms \n",
    "#If using the column, clean to remove the word in the bracket and convert the 14.0 to 14.\n",
    "label_freq = df['Number of Worms (non-counted)'].value_counts() / len(df)\n",
    "fig = label_freq.sort_values(ascending=False).plot.bar()\n",
    "\n",
    "fig.axhline(y=0.05, color='red')\n",
    "\n",
    "fig.set_ylabel('Percentage of items in each category')\n",
    "fig.set_xlabel('Variable: Num. of worms')\n",
    "fig.set_title('Rare Categories')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8eb7f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exploring Buzzer\n",
    "label_freq = df['Buzzer'].value_counts() / len(df)\n",
    "fig = label_freq.sort_values(ascending=False).plot.bar()\n",
    "\n",
    "fig.axhline(y=0.05, color='red')\n",
    "\n",
    "fig.set_ylabel('Percentage of items in each category')\n",
    "fig.set_xlabel('Variable: Buzzer')\n",
    "fig.set_title('Rare Categories')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca139eb",
   "metadata": {},
   "source": [
    "### Exploring float variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238e39b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop rows with no npk values\n",
    "df = df.dropna(subset=['Nitrogen01']) \n",
    "\n",
    "# Convert List of columns to float\n",
    "columns_to_convert = ['Phosphorous01', 'Phosphorous02', 'Nitrogen01', 'Nitrogen02', \n",
    "                      'Potassium01', 'Potassium02', 'Temp01', 'Hum01', 'Heat01', \n",
    "                      'SoilM01', 'SoilM02']\n",
    "\n",
    "# Convert columns to float\n",
    "for col in columns_to_convert:\n",
    "    # Convert non-positive values (such as \"?\" and negative numbers) with NaN\n",
    "    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee274c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3f6e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def diagnostic_plots(df, variable):\n",
    "    \n",
    "    plt.figure(figsize=(15,6))\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.rcParams[\"patch.edgecolor\"] = \"none\"\n",
    "    sns.histplot(df[variable], bins=30, kde = True, stat =\"density\", kde_kws=dict(cut = 3),alpha = 0.30)\n",
    "    plt.title('Histogram')\n",
    "\n",
    "    plt.subplot(1, 3, 2)\n",
    "    stats.probplot(df[variable], dist=\"norm\", plot=plt)\n",
    "    \n",
    "    plt.subplot(1, 3, 3)\n",
    "    sns.boxplot(y=df[variable])\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "    # Count outliers using IQR method\n",
    "    Q1 = df[variable].quantile(0.25)\n",
    "    Q3 = df[variable].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "    outliers = df[(df[variable] < lower_bound) | (df[variable] > upper_bound)]\n",
    "\n",
    "    print(f'Number of outliers in {variable}: {len(outliers)}')\n",
    "    \n",
    "diagnostic_plots(df, 'Phosphorous01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1dcbf0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "diagnostic_plots(df, 'Phosphorous02')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4536dbb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "diagnostic_plots(df, 'Nitrogen01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2be116",
   "metadata": {},
   "outputs": [],
   "source": [
    "diagnostic_plots(df, 'Nitrogen02')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021b8245",
   "metadata": {},
   "outputs": [],
   "source": [
    "diagnostic_plots(df, 'Potassium01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a91ced3",
   "metadata": {},
   "outputs": [],
   "source": [
    "diagnostic_plots(df, 'Potassium02')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091451b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "diagnostic_plots(df, 'Temp01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce26c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "diagnostic_plots(df, 'Hum01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d15eaad",
   "metadata": {},
   "outputs": [],
   "source": [
    "diagnostic_plots(df, 'Heat01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb565f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "diagnostic_plots(df, 'SoilM01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f897c98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "diagnostic_plots(df, 'SoilM02')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c101c4",
   "metadata": {},
   "source": [
    "### Relationship between variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d81e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Understanding the correlation of variables by showing the correlation matrix on a heatmap \n",
    "df_num = df.select_dtypes(['float64']).copy()\n",
    "\n",
    "# Compute the correlation matrix\n",
    "correlation_matrix = df_num.corr()\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "plt.figure(figsize=(16, 16))\n",
    "\n",
    "# Create a heatmap using seaborn\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', linewidths=.5)\n",
    "\n",
    "# Show the plot\n",
    "plt.title('Correlation Heatmap of Macpherson Rack 01 Numerical Features')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1c318c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate summary statistics for each feature\n",
    "summary_stats = {}\n",
    "for column in df_num:\n",
    "    stats = df.groupby('Buzzer')[column].agg(['mean', 'median', 'std'])\n",
    "    summary_stats[column] = stats\n",
    "\n",
    "# Display summary statistics for each feature\n",
    "for column, stats in summary_stats.items():\n",
    "    print(f\"Summary statistics for '{column}':\")\n",
    "    print(stats)\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708eb13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploring relationship between variables\n",
    "#Phosphorus\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(df['Phosphorous01'], df['Phosphorous02'])\n",
    "plt.title('Scatter Plot of Phosphorous01 vs Phosphorous02')\n",
    "plt.xlabel('Phosphorous01')\n",
    "plt.ylabel('Phosphorous02')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e816d4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Nitrogen\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(df['Nitrogen01'], df['Nitrogen02'])\n",
    "plt.title('Scatter Plot of Nitrogen01 vs Nitrogen02')\n",
    "plt.xlabel('Nitrogen01')\n",
    "plt.ylabel('Nitrogen02')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64b9624",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Potassium\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(df['Potassium01'], df['Potassium02'])\n",
    "plt.title('Scatter Plot of Potassium01 vs Potassium02')\n",
    "plt.xlabel('Potassium01')\n",
    "plt.ylabel('Potassium02')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c2a163",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Temp01 vs Heat01\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(df['Temp01'], df['Heat01'])\n",
    "plt.title('Scatter Plot of Temp01 vs Heat01')\n",
    "plt.xlabel('Temp01')\n",
    "plt.ylabel('Heat01')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a81f266",
   "metadata": {},
   "source": [
    "### 3. Cleaning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a04e49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping irrelevant features \n",
    "df = df.drop(columns=['Number of Worms (non-counted)', 'Light Intensity', 'Grams of Compost Harvested',\n",
    "                                'Buzzer', 'Compost Notes', 'pH Rod 1', 'pH Rod 2'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71297fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping rows with null npk values\n",
    "df['Timestamp']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "071e562b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'Timestamp' column to datetime\n",
    "df['Timestamp'] = pd.to_datetime(df['Timestamp'], format='%d %b %Y, %I:%M %p')\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946ac742",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating features with timestamp\n",
    "df['Time'] = df['Timestamp'].dt.time\n",
    "df['Day'] = df['Timestamp'].dt.day\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42701bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca9167e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb6d853d",
   "metadata": {},
   "source": [
    "### Treating missing and inaccurate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33c4c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))  # Set the figure size\n",
    "\n",
    "test = df[df['Day'] <= 6]\n",
    "# Plot phosphorus01 against time\n",
    "plt.plot(test['Timestamp'], test['Phosphorous01'], color='blue', linestyle='-')\n",
    "\n",
    "# Set labels and title\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Phosphorus01')\n",
    "plt.title('Phosphorus01 over Time')\n",
    "\n",
    "# Rotate x-axis labels for better readability if needed\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Show grid\n",
    "plt.grid(True)\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa938bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the time period of sensor malfunction\n",
    "start_time = pd.to_datetime('13:33:00').time()\n",
    "end_time = pd.to_datetime('13:38:00').time()\n",
    "\n",
    "# Flag the data points where day is 6th or 14th and time is between 13:33:00 and 13:38:00\n",
    "sensor_malfunction = ((df['Day'] == 6) | (df['Day'] == 14)) & (df['Time'].apply(lambda x: x >= start_time) & (df['Time'].apply(lambda x: x <= end_time)))\n",
    "\n",
    "NPK_columns = ['Phosphorous01', 'Phosphorous02', 'Nitrogen01', 'Nitrogen02', 'Potassium01' ,'Potassium02']\n",
    "for column in NPK_columns:\n",
    "    # Convert negative values to NaN\n",
    "    df.loc[df[column] <= 0, column] = np.nan\n",
    "    df.loc[sensor_malfunction, column] = np.nan\n",
    "    \n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b56313",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test moving average function\n",
    "def moving_average(df):\n",
    "    data = df.copy()\n",
    "    for column in columns_to_convert:\n",
    "        window_size = 3\n",
    "        moving_avg = data[column].rolling(window=window_size, min_periods=1).mean()\n",
    "\n",
    "        data[column] = np.where(data[column].isnull(), moving_avg, data[column])\n",
    "\n",
    "    plt.figure(figsize=(12,8))\n",
    "\n",
    "    test = data[data['Day'] <= 6]\n",
    "\n",
    "    plt.plot(test['Timestamp'], test['Phosphorous01'], color='blue', linestyle='-')\n",
    "\n",
    "    # Set labels and title\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Phosphorus01')\n",
    "    plt.title('Phosphorus01 over Time')\n",
    "\n",
    "    plt.xticks(rotation=45)\n",
    "\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.show()\n",
    "    return data\n",
    "\n",
    "avg = moving_average(df)\n",
    "avg.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d662c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test interpolation\n",
    "def interpolation(df):\n",
    "    data = df.copy()\n",
    "    for column in columns_to_convert:\n",
    "        data[column] = data[column].interpolate(method='linear')\n",
    "    plt.figure(figsize=(12,8))\n",
    "\n",
    "    test = data[data['Day'] <= 6]\n",
    "    plt.plot(test['Timestamp'], test['Phosphorous01'], color='blue', linestyle='-')\n",
    "\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Phosphorus01')\n",
    "    plt.title('Phosphorus01 over Time')\n",
    "\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.show()\n",
    "    return data\n",
    "\n",
    "interpolate = interpolation(df)\n",
    "interpolate.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9679acb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test forward and backward fill \n",
    "def fill(df):\n",
    "    data = df.copy()\n",
    "    for column in columns_to_convert:\n",
    "        data[column] = data[column].fillna(method='ffill').fillna(method='bfill')\n",
    "    plt.figure(figsize=(12,8))\n",
    "\n",
    "    test = data[data['Day'] <= 6]\n",
    "\n",
    "    plt.plot(test['Timestamp'], test['Phosphorous01'], color='blue', linestyle='-')\n",
    "\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Phosphorus01')\n",
    "    plt.title('Phosphorus01 over Time')\n",
    "\n",
    "    plt.xticks(rotation=45)\n",
    "\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.show()\n",
    "    return data\n",
    "\n",
    "filled = fill(df)\n",
    "filled.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6051cf2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chose interpolation in the end\n",
    "df = interpolation(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d60a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34423a66",
   "metadata": {},
   "source": [
    "### 4. Getting average NPK and Moisture values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03324c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Mean_Phosphorous'] = df[['Phosphorous01', 'Phosphorous02']].mean(axis=1)\n",
    "df['Mean_Nitrogen'] = df[['Nitrogen01', 'Nitrogen02']].mean(axis=1)\n",
    "df['Mean_Potassium'] = df[['Potassium01', 'Potassium02']].mean(axis=1)\n",
    "df['Mean_Moisture'] = df[['SoilM01', 'SoilM02']].mean(axis=1)\n",
    "df['Mean_Moisture'] = df['Mean_Moisture'].replace(0.5, 1)\n",
    "df = df.drop(columns = NPK_columns)\n",
    "df = df.drop(columns = ['SoilM01', 'SoilM02'])\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d891f566",
   "metadata": {},
   "source": [
    "### 5. Preparing for Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5a0cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=['Timestamp', \"Time\", \"Day\", \"Heat01\"])\n",
    "for col in df.columns:\n",
    "    df[col] = pd.to_numeric(df[col], errors='raise')\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4a5e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix = df.corr()\n",
    "colormap = plt.cm.BrBG\n",
    "plt.figure(figsize=(16, 16))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap=\"coolwarm\", linewidths=.5)\n",
    "plt.title('Correlation Heatmap of Macpherson Rack 01 Numerical Features')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23e029f",
   "metadata": {},
   "outputs": [],
   "source": [
    "nitro = df.copy()\n",
    "nitro = nitro.drop(columns=['Mean_Phosphorous', 'Mean_Potassium'])\n",
    "target_corr = nitro.corr()[\"Mean_Nitrogen\"].sort_values()\n",
    "corr_df = pd.DataFrame({\"Mean_Nitrogen\": target_corr})\n",
    "plt.figure(figsize=(15,15))\n",
    "plt.title('Correlation to Nitrogen', size=20)\n",
    "ax = sns.heatmap(corr_df, cmap=\"coolwarm\", annot=True)\n",
    "bottom, top = ax.get_ylim()\n",
    "ax.set_ylim(bottom + 0.5, top - 0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6482d7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "phos = df.copy()\n",
    "phos = phos.drop(columns=['Mean_Nitrogen', 'Mean_Potassium'])\n",
    "target_corr = phos.corr()[\"Mean_Phosphorous\"].sort_values()\n",
    "corr_df = pd.DataFrame({\"Mean_Phosphorous\": target_corr})\n",
    "plt.figure(figsize=(15,15))\n",
    "plt.title('Correlation to Phosphorous', size=20)\n",
    "ax = sns.heatmap(corr_df, cmap=\"coolwarm\", annot=True)\n",
    "bottom, top = ax.get_ylim()\n",
    "ax.set_ylim(bottom + 0.5, top - 0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0988ac17",
   "metadata": {},
   "outputs": [],
   "source": [
    "pot = df.copy()\n",
    "pot = pot.drop(columns=['Mean_Nitrogen', 'Mean_Phosphorous'])\n",
    "target_corr = pot.corr()[\"Mean_Potassium\"].sort_values()\n",
    "corr_df = pd.DataFrame({\"Mean_Potassium\": target_corr})\n",
    "plt.figure(figsize=(15,15))\n",
    "plt.title('Correlation to Potassium', size=20)\n",
    "ax = sns.heatmap(corr_df, cmap=\"coolwarm\", annot=True)\n",
    "bottom, top = ax.get_ylim()\n",
    "ax.set_ylim(bottom + 0.5, top - 0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c65e8d2",
   "metadata": {},
   "source": [
    "### 6. Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63305769",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scaled = df.copy()\n",
    "df_scaled = df_scaled.drop(columns=['Mean_Nitrogen', 'Mean_Potassium', 'Mean_Phosphorous'])\n",
    "scaler = StandardScaler()\n",
    "df_scaled = scaler.fit_transform(df_scaled)\n",
    "df_scaled = pd.DataFrame(df_scaled, columns = ['Temperature', 'Humidity', 'Moisture Presence'])\n",
    "df_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ed6286",
   "metadata": {},
   "source": [
    "### 7. Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9459fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df[['Mean_Nitrogen', 'Mean_Phosphorous', 'Mean_Potassium']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64e8635",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df_scaled, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f084e136",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb954c9",
   "metadata": {},
   "source": [
    "#### Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c17d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {}\n",
    "for nutrient in y_train.columns:\n",
    "    model = RandomForestRegressor(random_state=42)\n",
    "    model.fit(X_train, y_train[nutrient])\n",
    "    models[nutrient] = model\n",
    "\n",
    "# Testing models\n",
    "y_pred = pd.DataFrame()\n",
    "for nutrient, model in models.items():\n",
    "    y_pred[nutrient] = model.predict(X_test)\n",
    "\n",
    "# Evaluating models\n",
    "for nutrient, model in models.items():\n",
    "    mae = mean_absolute_error(y_test[nutrient], y_pred[nutrient])\n",
    "    r2 = r2_score(y_test[nutrient], y_pred[nutrient])\n",
    "    print(f'Mean Absolute Error for {nutrient}: {mae}')\n",
    "    print(f'R-squared for {nutrient}: {r2}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e5c532",
   "metadata": {},
   "source": [
    "#### GridSearch #1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b74319",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [None, 5, 10],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Dictionary to store best models\n",
    "best_models = {}\n",
    "\n",
    "# Train separate Random Forest models for each nutrient with GridSearchCV\n",
    "for nutrient in y_train.columns:\n",
    "    model = RandomForestRegressor(random_state=42)\n",
    "    grid_search = GridSearchCV(estimator=model, param_grid=param_grid, scoring='neg_mean_absolute_error', cv=5)\n",
    "    grid_search.fit(X_train, y_train[nutrient])\n",
    "    best_models[nutrient] = grid_search.best_estimator_\n",
    "    print(f\"Best parameters for {nutrient}: {grid_search.best_params_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45685044",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best hyperparameters for each nutrient\n",
    "best_params = {\n",
    "    'Mean_Nitrogen': {'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 100},\n",
    "    'Mean_Phosphorous': {'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 100},\n",
    "    'Mean_Potassium': {'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 300}\n",
    "}\n",
    "\n",
    "# Instantiate separate SVR models for each nutrient with best parameters\n",
    "models = {}\n",
    "for nutrient in y_train.columns:\n",
    "    model = RandomForestRegressor(**best_params[nutrient], random_state=42)\n",
    "    model.fit(X_train, y_train[nutrient])\n",
    "    models[nutrient] = model\n",
    "\n",
    "# Evaluate the models\n",
    "y_pred = pd.DataFrame()\n",
    "for nutrient, model in models.items():\n",
    "    y_pred[nutrient] = model.predict(X_test)\n",
    "\n",
    "# Evaluate the models\n",
    "for nutrient, model in models.items():\n",
    "    mae = mean_absolute_error(y_test[nutrient], y_pred[nutrient])\n",
    "    r2 = r2_score(y_test[nutrient], y_pred[nutrient])\n",
    "    print(f'Mean Absolute Error for {nutrient}: {mae}')\n",
    "    print(f'R-squared for {nutrient}: {r2}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8731fc50",
   "metadata": {},
   "source": [
    "#### GridSearch #2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33fdf927",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'n_estimators': [100, 250, 300],\n",
    "    'max_depth': [9,10,11],\n",
    "    'min_samples_split': [2, 3],\n",
    "    'min_samples_leaf': [1]\n",
    "}\n",
    "\n",
    "# Dictionary to store best models\n",
    "best_models = {}\n",
    "\n",
    "# Train separate Random Forest models for each nutrient with GridSearchCV\n",
    "for nutrient in y_train.columns:\n",
    "    model = RandomForestRegressor(random_state=42)\n",
    "    grid_search = GridSearchCV(estimator=model, param_grid=param_grid, scoring='neg_mean_absolute_error', cv=5)\n",
    "    grid_search.fit(X_train, y_train[nutrient])\n",
    "    best_models[nutrient] = grid_search.best_estimator_\n",
    "    print(f\"Best parameters for {nutrient}: {grid_search.best_params_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c785b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best hyperparameters for each nutrient\n",
    "best_params = {\n",
    "    'Mean_Nitrogen': {'max_depth': 9, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 250},\n",
    "    'Mean_Phosphorous': {'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 100},\n",
    "    'Mean_Potassium': {'max_depth': 9, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 300}\n",
    "}\n",
    "\n",
    "# Instantiate separate SVR models for each nutrient with best parameters\n",
    "models = {}\n",
    "for nutrient in y_train.columns:\n",
    "    model = RandomForestRegressor(**best_params[nutrient], random_state=42)\n",
    "    model.fit(X_train, y_train[nutrient])\n",
    "    models[nutrient] = model\n",
    "\n",
    "# Evaluate the models\n",
    "y_pred = pd.DataFrame()\n",
    "for nutrient, model in models.items():\n",
    "    y_pred[nutrient] = model.predict(X_test)\n",
    "\n",
    "# Evaluate the models\n",
    "for nutrient, model in models.items():\n",
    "    mae = mean_absolute_error(y_test[nutrient], y_pred[nutrient])\n",
    "    r2 = r2_score(y_test[nutrient], y_pred[nutrient])\n",
    "    print(f'Mean Absolute Error for {nutrient}: {mae}')\n",
    "    print(f'R-squared for {nutrient}: {r2}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fddf108",
   "metadata": {},
   "source": [
    "#### Gridsearch #3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dcd4a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'n_estimators': [100, 150, 250, 300],\n",
    "    'max_depth': [9,10,11],\n",
    "    'min_samples_split': [2, 3],\n",
    "    'min_samples_leaf': [1]\n",
    "}\n",
    "\n",
    "# Dictionary to store best models\n",
    "best_models = {}\n",
    "\n",
    "# Train separate Random Forest models for each nutrient with GridSearchCV\n",
    "for nutrient in y_train.columns:\n",
    "    model = RandomForestRegressor(random_state=42)\n",
    "    grid_search = GridSearchCV(estimator=model, param_grid=param_grid, scoring='neg_mean_absolute_error', cv=5)\n",
    "    grid_search.fit(X_train, y_train[nutrient])\n",
    "    best_models[nutrient] = grid_search.best_estimator_\n",
    "    print(f\"Best parameters for {nutrient}: {grid_search.best_params_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4119835",
   "metadata": {},
   "source": [
    "#### GridSearch #4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9cf81d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'n_estimators': [100, 250, 275, 300],\n",
    "    'max_depth': [9,10,11],\n",
    "    'min_samples_split': [2, 3],\n",
    "    'min_samples_leaf': [1]\n",
    "}\n",
    "\n",
    "# Dictionary to store best models\n",
    "best_models = {}\n",
    "\n",
    "# Train separate Random Forest models for each nutrient with GridSearchCV\n",
    "for nutrient in y_train.columns:\n",
    "    model = RandomForestRegressor(random_state=42)\n",
    "    grid_search = GridSearchCV(estimator=model, param_grid=param_grid, scoring='neg_mean_absolute_error', cv=5)\n",
    "    grid_search.fit(X_train, y_train[nutrient])\n",
    "    best_models[nutrient] = grid_search.best_estimator_\n",
    "    print(f\"Best parameters for {nutrient}: {grid_search.best_params_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00cc099c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best hyperparameters for each nutrient\n",
    "best_params = {\n",
    "    'Mean_Nitrogen': {'max_depth': 9, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 275},\n",
    "    'Mean_Phosphorous': {'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 100},\n",
    "    'Mean_Potassium': {'max_depth': 9, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 300}\n",
    "}\n",
    "\n",
    "# Instantiate separate SVR models for each nutrient with best parameters\n",
    "models = {}\n",
    "for nutrient in y_train.columns:\n",
    "    model = RandomForestRegressor(**best_params[nutrient], random_state=42)\n",
    "    model.fit(X_train, y_train[nutrient])\n",
    "    models[nutrient] = model\n",
    "\n",
    "# Evaluate the models\n",
    "y_pred = pd.DataFrame()\n",
    "for nutrient, model in models.items():\n",
    "    y_pred[nutrient] = model.predict(X_test)\n",
    "\n",
    "# Evaluate the models\n",
    "for nutrient, model in models.items():\n",
    "    mae = mean_absolute_error(y_test[nutrient], y_pred[nutrient])\n",
    "    r2 = r2_score(y_test[nutrient], y_pred[nutrient])\n",
    "    print(f'Mean Absolute Error for {nutrient}: {mae}')\n",
    "    print(f'R-squared for {nutrient}: {r2}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdbf4ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = pd.DataFrame()\n",
    "for nutrient, model in models.items():\n",
    "    y_pred[nutrient] = model.predict(X_test)\n",
    "\n",
    "# Combine predictions (e.g., simple averaging)\n",
    "ensemble_pred = y_pred.mean(axis=1)\n",
    "\n",
    "# Evaluate the ensemble model\n",
    "mae = mean_absolute_error(y_test.mean(axis=1), ensemble_pred)\n",
    "r2 = r2_score(y_test.mean(axis=1), ensemble_pred)\n",
    "print(f'Mean Absolute Error for the ensemble model: {mae}')\n",
    "print(f'R-squared for the ensemble model: {r2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05749c5b",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e72cad0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {}\n",
    "for nutrient in y_train.columns:\n",
    "    model = XGBRegressor(random_state=42)\n",
    "    model.fit(X_train, y_train[nutrient])\n",
    "    models[nutrient] = model\n",
    "\n",
    "# Testing models\n",
    "y_pred = pd.DataFrame()\n",
    "for nutrient, model in models.items():\n",
    "    y_pred[nutrient] = model.predict(X_test)\n",
    "\n",
    "# Evaluating models\n",
    "for nutrient, model in models.items():\n",
    "    mae = mean_absolute_error(y_test[nutrient], y_pred[nutrient])\n",
    "    r2 = r2_score(y_test[nutrient], y_pred[nutrient])\n",
    "    print(f'Mean Absolute Error for {nutrient}: {mae}')\n",
    "    print(f'R-squared for {nutrient}: {r2}\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab45b1b",
   "metadata": {},
   "source": [
    "#### GridSearch #1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53c79eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [None, 5, 10],\n",
    "    'eta': [0.3, 0.5, 0.7, 1],\n",
    "    'subsample': [0.3, 0.5, 0.7, 1]\n",
    "}\n",
    "\n",
    "# Dictionary to store best models\n",
    "best_models = {}\n",
    "\n",
    "# Train separate Random Forest models for each nutrient with GridSearchCV\n",
    "for nutrient in y_train.columns:\n",
    "    model = XGBRegressor(random_state=42)\n",
    "    grid_search = GridSearchCV(estimator=model, param_grid=param_grid, scoring='neg_mean_absolute_error', cv=5)\n",
    "    grid_search.fit(X_train, y_train[nutrient])\n",
    "    best_models[nutrient] = grid_search.best_estimator_\n",
    "    print(f\"Best parameters for {nutrient}: {grid_search.best_params_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ac382a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best hyperparameters for each nutrient\n",
    "best_params = {\n",
    "    'Mean_Nitrogen': {'max_depth': 10, 'eta': 0.3, 'n_estimators': 100, 'subsample': 1},\n",
    "    'Mean_Phosphorous': {'max_depth': 5, 'eta': 0.3, 'n_estimators': 100, 'subsample': 1},\n",
    "    'Mean_Potassium': {'max_depth': 10, 'eta': 0.3, 'n_estimators': 100, 'subsample': 1}\n",
    "}\n",
    "\n",
    "# Instantiate separate XGBoost models for each nutrient with best parameters\n",
    "models = {}\n",
    "for nutrient in y_train.columns:\n",
    "    model = XGBRegressor(**best_params[nutrient], random_state=42)\n",
    "    model.fit(X_train, y_train[nutrient])\n",
    "    models[nutrient] = model\n",
    "\n",
    "# Evaluate the models\n",
    "y_pred = pd.DataFrame()\n",
    "for nutrient, model in models.items():\n",
    "    y_pred[nutrient] = model.predict(X_test)\n",
    "\n",
    "# Evaluate the models\n",
    "for nutrient, model in models.items():\n",
    "    mae = mean_absolute_error(y_test[nutrient], y_pred[nutrient])\n",
    "    r2 = r2_score(y_test[nutrient], y_pred[nutrient])\n",
    "    print(f'Mean Absolute Error for {nutrient}: {mae}')\n",
    "    print(f'R-squared for {nutrient}: {r2}\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2a1694",
   "metadata": {},
   "source": [
    "#### GridSearch #2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3dccbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [5,7,10],\n",
    "    'eta': [0.2,0.3,0.4],\n",
    "    'subsample': [0.9, 1]\n",
    "}\n",
    "\n",
    "# Dictionary to store best models\n",
    "best_models = {}\n",
    "\n",
    "# Train separate Random Forest models for each nutrient with GridSearchCV\n",
    "for nutrient in y_train.columns:\n",
    "    model = XGBRegressor(random_state=42)\n",
    "    grid_search = GridSearchCV(estimator=model, param_grid=param_grid, scoring='neg_mean_absolute_error', cv=5)\n",
    "    grid_search.fit(X_train, y_train[nutrient])\n",
    "    best_models[nutrient] = grid_search.best_estimator_\n",
    "    print(f\"Best parameters for {nutrient}: {grid_search.best_params_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d055435",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best hyperparameters for each nutrient\n",
    "best_params = {\n",
    "    'Mean_Nitrogen': {'max_depth': 7, 'eta': 0.2, 'n_estimators': 100, 'subsample': 1},\n",
    "    'Mean_Phosphorous': {'max_depth': 7, 'eta': 0.2, 'n_estimators': 100, 'subsample': 1},\n",
    "    'Mean_Potassium': {'max_depth': 10, 'eta': 0.3, 'n_estimators': 100, 'subsample': 1}\n",
    "}\n",
    "\n",
    "# Instantiate separate XGBoost models for each nutrient with best parameters\n",
    "models = {}\n",
    "for nutrient in y_train.columns:\n",
    "    model = XGBRegressor(**best_params[nutrient], random_state=42)\n",
    "    model.fit(X_train, y_train[nutrient])\n",
    "    models[nutrient] = model\n",
    "\n",
    "# Evaluate the models\n",
    "y_pred = pd.DataFrame()\n",
    "for nutrient, model in models.items():\n",
    "    y_pred[nutrient] = model.predict(X_test)\n",
    "\n",
    "# Evaluate the models\n",
    "for nutrient, model in models.items():\n",
    "    mae = mean_absolute_error(y_test[nutrient], y_pred[nutrient])\n",
    "    r2 = r2_score(y_test[nutrient], y_pred[nutrient])\n",
    "    print(f'Mean Absolute Error for {nutrient}: {mae}')\n",
    "    print(f'R-squared for {nutrient}: {r2}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8942106",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'n_estimators': [50,100,150],\n",
    "    'max_depth': [6,8,10],\n",
    "    'eta': [0.1, 0.2,0.3],\n",
    "    'subsample': [1]\n",
    "}\n",
    "\n",
    "# Dictionary to store best models\n",
    "best_models = {}\n",
    "\n",
    "# Train separate Random Forest models for each nutrient with GridSearchCV\n",
    "for nutrient in y_train.columns:\n",
    "    model = XGBRegressor(random_state=42)\n",
    "    grid_search = GridSearchCV(estimator=model, param_grid=param_grid, scoring='neg_mean_absolute_error', cv=5)\n",
    "    grid_search.fit(X_train, y_train[nutrient])\n",
    "    best_models[nutrient] = grid_search.best_estimator_\n",
    "    print(f\"Best parameters for {nutrient}: {grid_search.best_params_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570e1d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best hyperparameters for each nutrient\n",
    "best_params = {\n",
    "    'Mean_Nitrogen': {'max_depth': 7, 'eta': 0.1, 'n_estimators': 100, 'subsample': 1},\n",
    "    'Mean_Phosphorous': {'max_depth': 7, 'eta': 0.1, 'n_estimators': 100, 'subsample': 1},\n",
    "    'Mean_Potassium': {'max_depth': 8, 'eta': 0.1, 'n_estimators': 100, 'subsample': 1}\n",
    "}\n",
    "\n",
    "# Instantiate separate XGBoost models for each nutrient with best parameters\n",
    "models = {}\n",
    "for nutrient in y_train.columns:\n",
    "    model = XGBRegressor(**best_params[nutrient], random_state=42)\n",
    "    model.fit(X_train, y_train[nutrient])\n",
    "    models[nutrient] = model\n",
    "\n",
    "# Evaluate the models\n",
    "y_pred = pd.DataFrame()\n",
    "for nutrient, model in models.items():\n",
    "    y_pred[nutrient] = model.predict(X_test)\n",
    "\n",
    "# Evaluate the models\n",
    "for nutrient, model in models.items():\n",
    "    mae = mean_absolute_error(y_test[nutrient], y_pred[nutrient])\n",
    "    r2 = r2_score(y_test[nutrient], y_pred[nutrient])\n",
    "    print(f'Mean Absolute Error for {nutrient}: {mae}')\n",
    "    print(f'R-squared for {nutrient}: {r2}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2829a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'n_estimators': [90, 100,110],\n",
    "    'max_depth': [7,8,9],\n",
    "    'eta': [0, 0.05, 0.1],\n",
    "    'subsample': [1]\n",
    "}\n",
    "\n",
    "# Dictionary to store best models\n",
    "best_models = {}\n",
    "\n",
    "# Train separate Random Forest models for each nutrient with GridSearchCV\n",
    "for nutrient in y_train.columns:\n",
    "    model = XGBRegressor(random_state=42)\n",
    "    grid_search = GridSearchCV(estimator=model, param_grid=param_grid, scoring='neg_mean_absolute_error', cv=5)\n",
    "    grid_search.fit(X_train, y_train[nutrient])\n",
    "    best_models[nutrient] = grid_search.best_estimator_\n",
    "    print(f\"Best parameters for {nutrient}: {grid_search.best_params_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc9b2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best hyperparameters for each nutrient\n",
    "best_params = {\n",
    "    'Mean_Nitrogen': {'max_depth': 7, 'eta': 0.1, 'n_estimators': 110, 'subsample': 1},\n",
    "    'Mean_Phosphorous': {'max_depth': 7, 'eta': 0.1, 'n_estimators': 90, 'subsample': 1},\n",
    "    'Mean_Potassium': {'max_depth': 8, 'eta': 0.1, 'n_estimators': 90, 'subsample': 1}\n",
    "}\n",
    "\n",
    "# Instantiate separate XGBoost models for each nutrient with best parameters\n",
    "models = {}\n",
    "for nutrient in y_train.columns:\n",
    "    model = XGBRegressor(**best_params[nutrient], random_state=42)\n",
    "    model.fit(X_train, y_train[nutrient])\n",
    "    models[nutrient] = model\n",
    "\n",
    "# Evaluate the models\n",
    "y_pred = pd.DataFrame()\n",
    "for nutrient, model in models.items():\n",
    "    y_pred[nutrient] = model.predict(X_test)\n",
    "\n",
    "# Evaluate the models\n",
    "for nutrient, model in models.items():\n",
    "    mae = mean_absolute_error(y_test[nutrient], y_pred[nutrient])\n",
    "    r2 = r2_score(y_test[nutrient], y_pred[nutrient])\n",
    "    print(f'Mean Absolute Error for {nutrient}: {mae}')\n",
    "    print(f'R-squared for {nutrient}: {r2}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e239735e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = pd.DataFrame()\n",
    "for nutrient, model in models.items():\n",
    "    y_pred[nutrient] = model.predict(X_test)\n",
    "\n",
    "# Combine predictions (e.g., simple averaging)\n",
    "ensemble_pred = y_pred.mean(axis=1)\n",
    "\n",
    "# Evaluate the ensemble model\n",
    "mae = mean_absolute_error(y_test.mean(axis=1), ensemble_pred)\n",
    "r2 = r2_score(y_test.mean(axis=1), ensemble_pred)\n",
    "print(f'Mean Absolute Error for the ensemble model: {mae}')\n",
    "print(f'R-squared for the ensemble model: {r2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f391aa",
   "metadata": {},
   "source": [
    "### Isolation forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bfd2396",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.ensemble import IsolationForest\n",
    "# Scale the NPK deviation features\n",
    "scaler = MinMaxScaler()\n",
    "desired_ratio = {'Nitrogen': 2, 'Phosphorus': 1, 'Potassium': 1}\n",
    "# Scale NPK deviation features separately\n",
    "scaled_mean_nitrogen = scaler.fit_transform(df['Mean_Nitrogen'].values.reshape(-1, 1)).flatten()\n",
    "scaled_mean_phosphorus = scaler.fit_transform(df['Mean_Phosphorous'].values.reshape(-1, 1)).flatten()\n",
    "scaled_mean_potassium = scaler.fit_transform(df['Mean_Potassium'].values.reshape(-1, 1)).flatten()\n",
    "\n",
    "# Calculate NPK deviation using scaled mean values\n",
    "nitrogen_deviation = abs(scaled_mean_nitrogen - desired_ratio['Nitrogen'])\n",
    "phosphorus_deviation = abs(scaled_mean_phosphorus - desired_ratio['Phosphorus'])\n",
    "potassium_deviation = abs(scaled_mean_potassium - desired_ratio['Potassium'])\n",
    "\n",
    "# Combine scaled mean and deviation with environmental features\n",
    "combined_df = pd.DataFrame({\n",
    "    'Temp01': df['Temp01'],\n",
    "    'Hum01': df['Hum01'],\n",
    "    'Moisture': df['Mean_Moisture'],\n",
    "    'NPK_Nitrogen_Deviation': nitrogen_deviation,\n",
    "    'NPK_Phosphorus_Deviation': phosphorus_deviation,\n",
    "    'NPK_Potassium_Deviation': potassium_deviation\n",
    "})\n",
    "\n",
    "# Initialize and train the Isolation Forest model with combined data\n",
    "isolation_forest = IsolationForest(contamination=0.05, random_state=49)\n",
    "\n",
    "isolation_forest.fit(combined_df)\n",
    "\n",
    "\n",
    "# Predict outliers/anomalies (1 for inliers, -1 for outliers)\n",
    "predictions = isolation_forest.predict(combined_df)\n",
    "\n",
    "# Identify outliers/anomalies in the original DataFrame\n",
    "outliers = df[predictions == -1]\n",
    "inliers = df[predictions == 1]\n",
    "anomaly_scores = isolation_forest.decision_function(combined_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a6491c",
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb292e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers.describe() #78 outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903fcc44",
   "metadata": {},
   "outputs": [],
   "source": [
    "inliers = df[predictions == 1]\n",
    "inliers.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7016205f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "# Train your anomaly detection model (e.g., Isolation Forest) and generate anomaly scores\n",
    "\n",
    "# Identify top outliers based on anomaly scores\n",
    "sorted_indices = np.argsort(anomaly_scores)\n",
    "sorted_indices = sorted_indices[::-1]\n",
    "top_outlier_indices = sorted_indices[:1]\n",
    "top_outliers = combined_df.iloc[top_outlier_indices]\n",
    "\n",
    "# Calculate SHAP values for the selected outlier data points\n",
    "explainer = shap.Explainer(isolation_forest)  # Replace 'model' with your anomaly detection model\n",
    "shap_values = explainer.shap_values(top_outliers)  # Replace 'outlier_data' with your selected outlier data points\n",
    "\n",
    "# Visualize SHAP explanations for individual outlier predictions\n",
    "shap.summary_plot(shap_values, features=top_outliers)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
